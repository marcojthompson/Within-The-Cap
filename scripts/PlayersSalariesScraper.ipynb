{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603f188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header block to include all modules that must be imported ahead of time\n",
    "# Only needs to be run once per session, and each time a new module is added\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd # this will need to be installed via command line first\n",
    "import lxml # this will need to be installed via command line first(as well)\n",
    "import html5lib # this will need to be installed via command line first\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6ff18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request succeeded with status code 200\n",
      "[['Arizona Cardinals' 'NFC' 'West']\n",
      " ['Atlanta Falcons' 'NFC' 'South']\n",
      " ['Baltimore Ravens' 'AFC' 'North']\n",
      " ['Buffalo Bills' 'AFC' 'East']\n",
      " ['Carolina Panthers' 'NFC' 'South']\n",
      " ['Chicago Bears' 'NFC' 'North']\n",
      " ['Cincinnati Bengals' 'AFC' 'North']\n",
      " ['Cleveland Browns' 'AFC' 'North']\n",
      " ['Dallas Cowboys' 'NFC' 'East']\n",
      " ['Denver Broncos' 'AFC' 'West']\n",
      " ['Detroit Lions' 'NFC' 'North']\n",
      " ['Green Bay Packers' 'NFC' 'North']\n",
      " ['Houston Texans' 'AFC' 'South']\n",
      " ['Indianapolis Colts' 'AFC' 'South']\n",
      " ['Jacksonville Jaguars' 'AFC' 'South']\n",
      " ['Kansas City Chiefs' 'AFC' 'West']\n",
      " ['Las Vegas Raiders' 'AFC' 'West']\n",
      " ['Los Angeles Chargers' 'AFC' 'West']\n",
      " ['Los Angeles Rams' 'NFC' 'West']\n",
      " ['Miami Dolphins' 'AFC' 'East']\n",
      " ['Minnesota Vikings' 'NFC' 'North']\n",
      " ['New England Patriots' 'AFC' 'East']\n",
      " ['New Orleans Saints' 'NFC' 'South']\n",
      " ['New York Giants' 'NFC' 'East']\n",
      " ['New York Jets' 'AFC' 'East']\n",
      " ['Philadelphia Eagles' 'NFC' 'East']\n",
      " ['Pittsburgh Steelers' 'AFC' 'North']\n",
      " ['San Francisco 49ers' 'NFC' 'West']\n",
      " ['Seattle Seahawks' 'NFC' 'West']\n",
      " ['Tampa Bay Buccaneers' 'NFC' 'South']\n",
      " ['Tennessee Titans' 'AFC' 'South']\n",
      " ['Washington Commanders' 'NFC' 'East']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_667/577390278.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TeamsDFAFC.drop(labels = [4, 9, 14], inplace = True)\n",
      "/tmp/ipykernel_667/577390278.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TeamsDFNFC.drop(labels = [4, 9, 14], inplace = True)\n",
      "/tmp/ipykernel_667/577390278.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TeamsDFAFC.rename(columns = {'East': 'TeamName'}, inplace = True)\n",
      "/tmp/ipykernel_667/577390278.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TeamsDFNFC.rename(columns = {'East': 'TeamName'}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Method to retrieve all team names, confrences, and divisions \n",
    "\n",
    "def GetAllTeamNamesConfrencesDivisions():\n",
    "    # Use requests to get the raw HTML response from overthecap.com\n",
    "    CBSResponse = requests.get(f'https://www.cbssports.com/nfl/teams/')\n",
    "    \n",
    "    # If we get a normal response, proceed with data scraping\n",
    "    if CBSResponse.status_code == 200:\n",
    "        \n",
    "        # Confirm the request was successful\n",
    "        print(f\"Request succeeded with status code {CBSResponse.status_code}\")\n",
    "\n",
    "        # Use Pandas to read the HTML content and put it into a DataFrame\n",
    "        TableTag = 'TableBase-table'\n",
    "        DataFrames = pd.read_html(CBSResponse.content, attrs={'class': TableTag})\n",
    "        \n",
    "        # Select the specific parts of the dataframe we want \n",
    "        TeamsDFAFC = DataFrames[0][['East']]\n",
    "        TeamsDFNFC = DataFrames[1][['East']]\n",
    "        \n",
    "        # Insert a new row at the top with column name\n",
    "        TeamsDFAFC.insert(0, 'Conference', 'AFC')\n",
    "        TeamsDFNFC.insert(0, 'Conference', 'NFC')\n",
    "        \n",
    "        # Generate a list of divisions in the order the appear\n",
    "        DivisionsList = ['East', TeamsDFAFC.iloc[4, 1], TeamsDFAFC.iloc[9, 1], TeamsDFAFC.iloc[14, 1]]\n",
    "        \n",
    "        # The CBS table includes division labels as rows so they need to be removed\n",
    "        TeamsDFAFC.drop(labels = [4, 9, 14], inplace = True)\n",
    "        TeamsDFNFC.drop(labels = [4, 9, 14], inplace = True)\n",
    "        \n",
    "        # Rename the column with all the teamnames, concatinate the dataframes together\n",
    "        TeamsDFAFC.rename(columns = {'East': 'TeamName'}, inplace = True)\n",
    "        TeamsDFNFC.rename(columns = {'East': 'TeamName'}, inplace = True)\n",
    "        TeamsDF = pd.concat([TeamsDFAFC, TeamsDFNFC], ignore_index = True)\n",
    "        \n",
    "        # Create list to add each division to its respective team\n",
    "        DivisionsFullList = []\n",
    "        for i in range(2):\n",
    "            for division in DivisionsList:\n",
    "                for j in range(4):\n",
    "                    DivisionsFullList.append(division)\n",
    "        \n",
    "        # Add the new column to the teams dataframe\n",
    "        TeamsDF['Division'] = DivisionsFullList\n",
    "        \n",
    "        # Change the order of columns to match the database schema  \n",
    "        new_order = ['TeamName','Conference', 'Division']\n",
    "        TeamsDF = TeamsDF.loc[:, new_order]\n",
    "        \n",
    "        # Sort the teams dataframe by team name\n",
    "        TeamsDF.sort_values(by = ['TeamName'], inplace = True)\n",
    "        \n",
    "        # Turn the DataFrame into a numpy array and sort it by team name\n",
    "        TeamsArray = TeamsDF.to_numpy()\n",
    "        print(TeamsArray)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Read the csv back for testing purposes\n",
    "        TeamsDF = pd.read_csv(f'./Team_Lists/Teams-List.csv', header = None)\n",
    "        TeamsDF.columns = ['TeamName','Conference', 'Division']\n",
    "        print(TeamsDF)\n",
    "        \"\"\"\n",
    "\n",
    "    # If we don't get a normal reponse, stop scraping\n",
    "    else:\n",
    "        print(f\"Request failed with status code {CBSResponse.status_code}\")\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a87786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request succeeded with status code 200\n",
      "['ARI', 'ATL', 'BAL', 'BUF', 'CAR', 'CHI', 'CIN', 'CLE', 'DAL', 'DEN', 'DET', 'HOU', 'IND', 'JAC', 'LAC', 'MIA', 'MIN', 'NYG', 'NYJ', 'PHI', 'PIT', 'SEA', 'TEN', 'WAS']\n"
     ]
    }
   ],
   "source": [
    "# Method to get all team abbreviations\n",
    "def GetAllTeamAbbrvs():\n",
    "   # Define the API endpoint and parameters\n",
    "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": \"Wikipedia:WikiProject_National_Football_League/National_Football_League_team_abbreviations\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\"\n",
    "    }\n",
    "\n",
    "    # Make the request to the Wikipedia API\n",
    "    WikiResponse = requests.get(endpoint, params=params)\n",
    "    \n",
    "    # If we get a normal response, proceed with data scraping\n",
    "    if WikiResponse.status_code == 200:\n",
    "        \n",
    "        # Confirm the request was successful\n",
    "        print(f\"Request succeeded with status code {WikiResponse.status_code}\")\n",
    "        \n",
    "        # Extract the JSON data from the response\n",
    "        WikiData = WikiResponse.json()\n",
    "        PageContent = WikiData['query']['pages']['53669795']['revisions'][0]['*']\n",
    "        \n",
    "        # Sort the team abbreviations into a list, select only the abbreviations we want\n",
    "        AllAbbeviations = re.findall(r'\\b[A-Z]{3}\\b', PageContent)\n",
    "        TeamAbbrviations = AllAbbeviations[4::2] # This selects every other element in the list from index 4\n",
    "        print(TeamAbbrviations)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Error fetching data from the Wikipedia API.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f1d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the salary cap hits for every player currently signed on an NFL team\n",
    "# Creates a csv with columns player name, salary cap hit, and team location abbrviation \n",
    "\n",
    "def GetTeamSalaryCapHits(team):\n",
    "    # Use requests to get the raw HTML response from overthecap.com\n",
    "    OTCResponse = requests.get(f'https://overthecap.com/salary-cap/{team[0]}')\n",
    "        \n",
    "    # If we get a normal response proceed with data scraping\n",
    "    if OTCResponse.status_code == 200:\n",
    "        print(f\"Request succeeded with status code {OTCResponse.status_code}\")\n",
    "        TeamNameCaptalized = (team[0].replace(\"-\", \" \")).title()\n",
    "        print(f\"Here are the Salary Cap hits for all players who are currently signed with the {TeamNameCaptalized}:\")\n",
    "\n",
    "        # Pandas read_html method allows the table contents to be put into a DataFrame\n",
    "        TableTag = 'salary-cap-table contracted-players'\n",
    "        DataFrames = pd.read_html(OTCResponse.content, attrs = {'class': TableTag})\n",
    "        \n",
    "        # Select the specific parts of the dataframe we want \n",
    "        SalariesDF = DataFrames[0][['Player', 'Cap Number']]\n",
    "        if len(SalariesDF) >= 51:\n",
    "            SalariesDF = SalariesDF.drop(51, axis=0)\n",
    "        # Add team location abv to each player\n",
    "        SalariesDF = SalariesDF.assign(TeamLOC = team[1])\n",
    "        SalariesDF = SalariesDF.assign(TeamName = (team[0].replace(\"-\", \" \")).title())\n",
    "        \n",
    "        \n",
    "        # Change the order of columns to make more sense semantically \n",
    "        new_order = ['Player', 'TeamLOC', 'TeamName', 'Cap Number']\n",
    "        SalariesDF = SalariesDF.loc[:, new_order]\n",
    "        \n",
    "        # Specify filename and path\n",
    "        csvPathName = f'./Salary_Lists/{TeamNameCaptalized}-Player-Salary-List.csv'\n",
    "        \n",
    "        # Create a .csv file with all the player names and their cap hits\n",
    "        SalariesDF.to_csv(csvPathName, index = False, header = False)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        # Read the csv back for testing purposes\n",
    "        SalariesCSV = pd.read_csv(f'./Salary_Lists/{TeamNameCaptalized}-Player-Salary-List.csv', header = None)\n",
    "        SalariesCSV.columns = ['Player', 'TeamLOC', 'TeamName', 'Cap Number']\n",
    "        print(SalariesCSV)\n",
    "        \"\"\"\n",
    "    # If we don't get a normal reponse, stop scraping\n",
    "    else:\n",
    "        print(f\"Request failed with status code {OTCResponse.status_code}\")\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e70a908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request succeeded with status code 200\n",
      "Here are the Salary Cap hits for all players who are currently signed with the San Francisco 49Ers:\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'Salary_Lists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         GetTeamSalaryCapHits(team)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# GetMultipleTeamSalaryCapHits(NFLTeams)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mGetTeamSalaryCapHits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msan-francisco-49ers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mGetTeamSalaryCapHits\u001b[0;34m(team)\u001b[0m\n\u001b[1;32m     32\u001b[0m     csvPathName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Salary_Lists/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTeamNameCaptalized\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-Player-Salary-List.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Create a .csv file with all the player names and their cap hits\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mSalariesDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsvPathName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    # Read the csv back for testing purposes\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    SalariesCSV = pd.read_csv(f'./Salary_Lists/{TeamNameCaptalized}-Player-Salary-List.csv', header = None)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    SalariesCSV.columns = ['Player', 'TeamLOC', 'TeamName', 'Cap Number']\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    print(SalariesCSV)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# If we don't get a normal reponse, stop scraping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3761\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3764\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3765\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3770\u001b[0m )\n\u001b[0;32m-> 3772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3775\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1168\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1169\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    250\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    251\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 737\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    598\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'Salary_Lists'"
     ]
    }
   ],
   "source": [
    "# This particular cell will be used to call all other methods\n",
    "        \n",
    "GetAllTeamNamesConfrencesDivisions()     \n",
    "GetAllTeamAbbrvs()\n",
    "\n",
    "# A list of links to Over The Cap Salary Cap pages for various teams\n",
    "NFLTeams = [['buffalo-bills', 'BUF'], ['miami-dolphins', 'MIA'], ['new-england-patriots', 'NE'], ['new-york-jets', 'NYJ'],\n",
    "            ['baltimore-ravens', 'BAL'], ['cincinnati-bengals', 'CIN'], ['cleveland-browns', 'CLE'], ['pittsburgh-steelers', 'PIT'],\n",
    "            ['houston-texans', 'HOU'], ['indianapolis-colts', 'IND'], ['jacksonville-jaguars', 'JAX'], ['tennessee-titans', 'TEN'], \n",
    "            ['denver-broncos', 'DEN'], ['kansas-city-chiefs', 'KC'], ['las-vegas-raiders', 'LV'], ['los-angeles-chargers', 'LAC'],\n",
    "            ['dallas-cowboys', 'DAL'], ['new-york-giants', 'NYG'], ['philadelphia-eagles', 'PHI'], ['washington-commanders', 'WSH'],\n",
    "            ['chicago-bears', 'CHI'], ['detroit-lions', 'DET'], ['green-bay-packers', 'GB'], ['minnesota-vikings', 'MIN'],\n",
    "            ['atlanta-falcons', 'ATL'], ['carolina-panthers', 'CAR'], ['new-orleans-saints', 'NO'], ['tampa-bay-buccaneers', 'TB'],\n",
    "            ['arizona-cardinals', 'ARI'], ['los-angeles-rams', 'LAR'], ['san-francisco-49ers', 'SF'], ['seattle-seahawks', 'SEA']\n",
    "           ]\n",
    "\n",
    "# Call the method defined above to get all the player data\n",
    "def GetMultipleTeamSalaryCapHits(TeamList):\n",
    "    for team in TeamList:\n",
    "        GetTeamSalaryCapHits(team)\n",
    "\n",
    "# GetMultipleTeamSalaryCapHits(NFLTeams)\n",
    "GetTeamSalaryCapHits(['san-francisco-49ers', 'SF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7302f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
